#!/usr/bin/env python3
"""
Comprehensive test suite for DBBasic Channels and Task Queue
Tests performance, concurrency, reliability, and compares with PostgreSQL benchmarks
"""

import pytest
import asyncio
import time
import json
import uuid
from typing import List, Dict, Any
import duckdb
import statistics
from datetime import datetime, timedelta

# Import our systems
from dbbasic_channels import DBBasicChannels, Channel
from dbbasic_task_queue import DBBasicTaskQueue, TaskWorker


class TestDBBasicChannels:
    """Test suite for DBBasic Channels pub/sub system"""

    @pytest.fixture
    async def channels(self):
        """Create a test channels instance"""
        channels = DBBasicChannels(db_path=":memory:")  # In-memory for tests
        yield channels
        # Cleanup
        channels.conn.close()

    @pytest.mark.asyncio
    async def test_publish_single_message(self, channels):
        """Test publishing a single message"""
        message_id = await channels.publish(
            Channel.ORDERS.value,
            "new_order",
            {"order_id": "TEST123", "total": 100.00}
        )

        assert message_id is not None
        assert len(message_id) == 36  # UUID length

        # Verify in database
        result = channels.conn.execute(
            "SELECT * FROM messages WHERE id = ?", [message_id]
        ).fetchone()

        assert result is not None
        assert result[1] == Channel.ORDERS.value  # channel
        assert result[2] == "new_order"  # message_type

    @pytest.mark.asyncio
    async def test_subscribe_and_receive(self, channels):
        """Test subscribing to a channel and receiving messages"""
        received_messages = []

        async def test_callback(message):
            received_messages.append(message)

        # Subscribe to orders channel
        subscriber_id = await channels.subscribe(
            Channel.ORDERS.value,
            test_callback,
            "test_subscriber"
        )

        # Publish a message
        await channels.publish(
            Channel.ORDERS.value,
            "test_message",
            {"data": "test"}
        )

        # Give it time to process
        await asyncio.sleep(0.1)

        assert len(received_messages) == 1
        assert received_messages[0]['channel'] == Channel.ORDERS.value
        assert received_messages[0]['message_type'] == "test_message"

    @pytest.mark.asyncio
    async def test_wildcard_subscription(self, channels):
        """Test subscribing to all channels with wildcard"""
        all_messages = []

        async def monitor_callback(message):
            all_messages.append(message)

        # Subscribe to all channels
        await channels.subscribe("*", monitor_callback, "global_monitor")

        # Publish to different channels
        await channels.publish(Channel.ORDERS.value, "order", {"id": 1})
        await channels.publish(Channel.SHIPPING.value, "ship", {"id": 2})
        await channels.publish(Channel.FRAUD.value, "check", {"id": 3})

        await asyncio.sleep(0.1)

        # Should receive all messages
        assert len(all_messages) == 3
        channels_received = [msg['channel'] for msg in all_messages]
        assert Channel.ORDERS.value in channels_received
        assert Channel.SHIPPING.value in channels_received
        assert Channel.FRAUD.value in channels_received

    @pytest.mark.asyncio
    async def test_channel_worker_processing(self, channels):
        """Test worker watching and processing channel messages"""
        processed = []

        async def test_processor(message):
            processed.append(message)
            return {"status": "processed", "id": message['payload'].get('id')}

        # Start worker in background
        worker_task = asyncio.create_task(
            channels.watch_channel(
                Channel.ORDERS.value,
                "test_worker",
                test_processor
            )
        )

        # Give worker time to start
        await asyncio.sleep(0.1)

        # Publish messages
        for i in range(5):
            await channels.publish(
                Channel.ORDERS.value,
                "process_me",
                {"id": i, "data": f"test_{i}"}
            )

        # Wait for processing
        await asyncio.sleep(0.5)

        # Cancel worker
        worker_task.cancel()
        try:
            await worker_task
        except asyncio.CancelledError:
            pass

        # Check all were processed
        assert len(processed) == 5
        assert all(msg['message_type'] == "process_me" for msg in processed)

    @pytest.mark.asyncio
    async def test_message_ttl_expiration(self, channels):
        """Test that messages expire after TTL"""
        # Publish with short TTL
        message_id = await channels.publish(
            Channel.ORDERS.value,
            "expires_soon",
            {"data": "test"},
            ttl_seconds=1
        )

        # Wait for expiration
        await asyncio.sleep(2)

        # Try to get the message - should be expired
        result = channels.conn.execute("""
            SELECT * FROM messages
            WHERE id = ?
            AND (created_at + INTERVAL '1 second' * ttl_seconds) > CURRENT_TIMESTAMP
        """, [message_id]).fetchone()

        assert result is None  # Message should be expired

    @pytest.mark.asyncio
    async def test_concurrent_publishing(self, channels):
        """Test concurrent publishing from multiple coroutines"""

        async def publisher(channel, count):
            for i in range(count):
                await channels.publish(
                    channel,
                    f"msg_{i}",
                    {"index": i}
                )

        # Run multiple publishers concurrently
        tasks = [
            publisher(Channel.ORDERS.value, 10),
            publisher(Channel.SHIPPING.value, 10),
            publisher(Channel.FRAUD.value, 10)
        ]

        start_time = time.time()
        await asyncio.gather(*tasks)
        elapsed = time.time() - start_time

        # Check all messages were inserted
        count = channels.conn.execute(
            "SELECT COUNT(*) FROM messages"
        ).fetchone()[0]

        assert count == 30
        assert elapsed < 1.0  # Should be very fast

    @pytest.mark.asyncio
    async def test_channel_statistics(self, channels):
        """Test channel statistics tracking"""
        # Publish to various channels
        await channels.publish(Channel.ORDERS.value, "order", {})
        await channels.publish(Channel.ORDERS.value, "order", {})
        await channels.publish(Channel.SHIPPING.value, "ship", {})

        stats = channels.get_channel_stats()

        assert stats[Channel.ORDERS.value]['total_messages'] >= 2
        assert stats[Channel.SHIPPING.value]['total_messages'] >= 1


class TestDBBasicTaskQueue:
    """Test suite for DBBasic Task Queue"""

    @pytest.fixture
    def queue(self):
        """Create a test task queue instance"""
        queue = DBBasicTaskQueue(db_path=":memory:")
        yield queue
        queue.conn.close()

    def test_add_single_task(self, queue):
        """Test adding a single task"""
        task_id = queue.add_task(
            task_type="test_task",
            payload={"key": "value"},
            priority=5
        )

        assert task_id is not None

        # Verify in database
        result = queue.conn.execute(
            "SELECT * FROM task_queue WHERE id = ?", [task_id]
        ).fetchone()

        assert result is not None
        assert result[1] == "test_task"  # task_type
        assert result[4] == "pending"  # status

    def test_bulk_add_tasks(self, queue):
        """Test bulk adding tasks - this tests our speed claims!"""
        tasks = [
            {
                'task_type': 'bulk_test',
                'service_name': 'test_service',
                'payload': {'index': i},
                'priority': i % 10
            }
            for i in range(10000)
        ]

        start_time = time.time()
        count = queue.bulk_add_tasks(tasks)
        elapsed = time.time() - start_time

        tasks_per_second = count / elapsed

        assert count == 10000
        assert elapsed < 1.0  # Should be very fast
        print(f"\nâš¡ Bulk insert rate: {tasks_per_second:,.0f} tasks/second")

        # Verify all were inserted
        db_count = queue.conn.execute(
            "SELECT COUNT(*) FROM task_queue"
        ).fetchone()[0]

        assert db_count == 10000

    def test_get_next_task_priority(self, queue):
        """Test that tasks are retrieved by priority"""
        # Add tasks with different priorities
        low_id = queue.add_task("low", {}, priority=1)
        high_id = queue.add_task("high", {}, priority=10)
        medium_id = queue.add_task("medium", {}, priority=5)

        # Should get high priority first
        task = queue.get_next_task("worker1")
        assert task is not None
        assert task['task_type'] == "high"

        # Then medium
        task = queue.get_next_task("worker1")
        assert task['task_type'] == "medium"

        # Then low
        task = queue.get_next_task("worker1")
        assert task['task_type'] == "low"

    def test_task_locking(self, queue):
        """Test that locked tasks aren't given to other workers"""
        task_id = queue.add_task("test", {})

        # Worker 1 gets the task
        task1 = queue.get_next_task("worker1", lock_duration_seconds=60)
        assert task1 is not None

        # Worker 2 shouldn't get the same task
        task2 = queue.get_next_task("worker2")
        assert task2 is None  # No tasks available

    def test_complete_task(self, queue):
        """Test completing a task"""
        task_id = queue.add_task("test", {"data": "test"})

        # Get and complete the task
        task = queue.get_next_task("worker1")
        queue.complete_task(task['id'], result={"status": "done"})

        # Check status
        result = queue.conn.execute(
            "SELECT status, result FROM task_queue WHERE id = ?",
            [task['id']]
        ).fetchone()

        assert result[0] == "completed"
        assert json.loads(result[1])['status'] == "done"

    def test_fail_task_with_retry(self, queue):
        """Test failing a task with retry logic"""
        task_id = queue.add_task("test", {}, max_attempts=3)

        # Fail the task twice
        for attempt in range(2):
            task = queue.get_next_task("worker1")
            assert task is not None
            queue.fail_task(task['id'], f"Error {attempt}")

            # Check it goes back to pending
            status = queue.conn.execute(
                "SELECT status FROM task_queue WHERE id = ?", [task_id]
            ).fetchone()[0]
            assert status == "pending"

        # Third failure should mark as failed
        task = queue.get_next_task("worker1")
        queue.fail_task(task['id'], "Final error")

        status = queue.conn.execute(
            "SELECT status FROM task_queue WHERE id = ?", [task_id]
        ).fetchone()[0]
        assert status == "failed"

    def test_metrics_calculation(self, queue):
        """Test metrics calculation"""
        # Add various tasks
        for i in range(10):
            queue.add_task("test", {"index": i})

        # Process some
        for i in range(5):
            task = queue.get_next_task(f"worker_{i}")
            if task:
                queue.complete_task(task['id'])

        metrics = queue.get_metrics()

        assert metrics['pending_tasks'] == 5
        assert metrics['completed_tasks'] == 5
        assert metrics['theoretical_max_rows_sec'] == 402_000_000

    @pytest.mark.asyncio
    async def test_task_worker(self, queue):
        """Test the TaskWorker class"""
        processed = []

        async def test_processor(task):
            processed.append(task)
            return {"processed": True}

        # Create worker
        worker = TaskWorker("test_worker", queue)
        worker.process_task = test_processor

        # Add tasks
        for i in range(3):
            queue.add_task("test", {"index": i})

        # Run worker briefly
        worker_task = asyncio.create_task(worker.start())
        await asyncio.sleep(0.5)
        worker.stop()
        worker_task.cancel()
        try:
            await worker_task
        except asyncio.CancelledError:
            pass

        # Check tasks were processed
        assert len(processed) == 3


class TestPerformanceBenchmarks:
    """Performance benchmarks comparing with PostgreSQL baseline"""

    @pytest.fixture
    def queue(self):
        return DBBasicTaskQueue(db_path=":memory:")

    @pytest.fixture
    async def channels(self):
        return DBBasicChannels(db_path=":memory:")

    def test_benchmark_insert_speed(self, queue):
        """Benchmark: Insert speed vs PostgreSQL baseline"""
        counts = [100, 1000, 10000]
        results = []

        for count in counts:
            tasks = [
                {'task_type': 'bench', 'payload': {'i': i}}
                for i in range(count)
            ]

            # Use bulk insert
            start = time.time()
            queue.bulk_add_tasks(tasks)
            elapsed = time.time() - start

            rate = count / elapsed
            results.append({
                'count': count,
                'time': elapsed,
                'rate': rate
            })

            print(f"\nðŸ“Š {count:,} tasks: {elapsed:.3f}s = {rate:,.0f} tasks/sec")

        # PostgreSQL baseline: ~5,000-10,000 inserts/sec
        # Our target: 100,000+ inserts/sec
        assert all(r['rate'] > 10000 for r in results), "Should beat PostgreSQL baseline"

    @pytest.mark.asyncio
    async def test_benchmark_concurrent_workers(self, queue):
        """Benchmark: Concurrent worker processing"""

        # Add many tasks
        tasks = [
            {'task_type': 'work', 'payload': {'id': i}}
            for i in range(1000)
        ]
        queue.bulk_add_tasks(tasks)

        processed_count = 0

        async def worker_proc(worker_id):
            nonlocal processed_count
            while True:
                task = queue.get_next_task(worker_id)
                if not task:
                    break
                queue.complete_task(task['id'])
                processed_count += 1

        # Run multiple workers
        start = time.time()
        workers = [worker_proc(f"worker_{i}") for i in range(10)]
        await asyncio.gather(*workers)
        elapsed = time.time() - start

        rate = processed_count / elapsed
        print(f"\nðŸ“Š Processed {processed_count} tasks with 10 workers")
        print(f"   Time: {elapsed:.3f}s")
        print(f"   Rate: {rate:,.0f} tasks/sec")

        # Should process very quickly with concurrent workers
        assert rate > 500, "Should process >500 tasks/sec with 10 workers"

    @pytest.mark.asyncio
    async def test_benchmark_pub_sub_latency(self, channels):
        """Benchmark: Pub/Sub message latency"""
        latencies = []

        async def measure_latency():
            received_time = None

            async def callback(msg):
                nonlocal received_time
                received_time = time.time()

            await channels.subscribe(Channel.ORDERS.value, callback)

            # Measure latency
            send_time = time.time()
            await channels.publish(Channel.ORDERS.value, "test", {})

            # Wait for message
            await asyncio.sleep(0.01)

            if received_time:
                latency = (received_time - send_time) * 1000  # ms
                latencies.append(latency)

        # Run multiple measurements
        for _ in range(100):
            await measure_latency()

        avg_latency = statistics.mean(latencies)
        p99_latency = statistics.quantiles(latencies, n=100)[98]

        print(f"\nðŸ“Š Pub/Sub Latency:")
        print(f"   Average: {avg_latency:.3f}ms")
        print(f"   P99: {p99_latency:.3f}ms")

        # Should have very low latency
        assert avg_latency < 10, "Average latency should be <10ms"
        assert p99_latency < 50, "P99 latency should be <50ms"


def test_integration_full_pipeline():
    """Integration test: Complete pipeline from publish to process"""

    async def run_pipeline():
        channels = DBBasicChannels(db_path=":memory:")
        queue = DBBasicTaskQueue(db_path=":memory:")

        results = []

        # Setup: Channel subscriber that adds to task queue
        async def channel_to_queue(message):
            if message['message_type'] == 'new_order':
                queue.add_task(
                    'process_order',
                    message['payload'],
                    service_name='order_processor'
                )

        await channels.subscribe(Channel.ORDERS.value, channel_to_queue)

        # Publish order messages
        for i in range(10):
            await channels.publish(
                Channel.ORDERS.value,
                'new_order',
                {'order_id': f'ORD{i}', 'amount': i * 100}
            )

        await asyncio.sleep(0.1)  # Let messages propagate

        # Process tasks from queue
        for _ in range(10):
            task = queue.get_next_task('worker1')
            if task:
                queue.complete_task(task['id'], {'processed': True})
                results.append(task)

        # Verify pipeline worked
        assert len(results) == 10
        assert all(t['task_type'] == 'process_order' for t in results)

        # Check metrics
        metrics = queue.get_metrics()
        assert metrics['completed_tasks'] == 10

        print(f"\nâœ… Full pipeline test: {len(results)} orders processed")

    asyncio.run(run_pipeline())


if __name__ == "__main__":
    # Run tests with pytest
    pytest.main([__file__, "-v", "--tb=short"])

    # Or run specific benchmarks
    print("\n" + "="*60)
    print("Running Performance Benchmarks")
    print("="*60)

    queue = DBBasicTaskQueue(db_path=":memory:")

    # Benchmark 1: Bulk insert speed
    print("\nðŸ“Š Benchmark: Bulk Insert Speed")
    tasks = [{'task_type': 'bench', 'payload': {'i': i}} for i in range(100000)]
    start = time.time()
    queue.bulk_add_tasks(tasks)
    elapsed = time.time() - start
    print(f"   100,000 tasks inserted in {elapsed:.3f}s")
    print(f"   Rate: {100000/elapsed:,.0f} tasks/second")
    print(f"   vs PostgreSQL: ~{(100000/elapsed)/10000:.1f}x faster")

    print("\nðŸš€ DBBasic is ready to replace PostgreSQL queues!")